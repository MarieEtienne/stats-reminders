---
title: "Principal Component Analysis"
authors: "Kilian Rocher, Océane Picot, Alexia Orhan"
format: html
editor: visual
---

## Introduction

PCA (Principal Component Analysis) is a dimensionality reduction technique used in data analysis and machine learning. It helps you to reduce the number of features in a dataset while keeping the most important information (variance).

The main goals are to reduce the number of variables, to visualize high-dimensional data in 2D or 3D, to detect patterns, clusters and outliers, remove noise and redundant information.

For example, if you get a data set with 10 correlated variables, PCA can reduce it to 2 or 3 components that explains 90% of the variance and makes it easier to visualize.

To realize a PCA, you need first to standardize the data (center and scale variables), to show how the variables vary together with a covariance matrix, to calculate eigenvalues and eigenvectors, to sort the components by decreasing values (the first components explain the most variance) and to project the data on the new axes.

## Example's presentation : Eye size evolution in spider

Our example is based on the article "*Allometry and ecology shape eye size evolution in spiders* " (Chong et al., 2024, Current Biology 34, 3178–3188 July 22, 2024 ª 2024 The Authors. Published by Elsevier Inc. https://doi.org/10.1016/j.cub.2024.06.020). This study examines how eye size evolves in spiders, which possess up to four pairs of eyes of variable sizes. Researchers analyzed 1,098 individuals from 39 species and supplemented their data with approximately 474 additional species.

The researchers measured four different eye pairs simultaneously. PCA reduces this 4-dimensional dataset into interpretable components, making it easier to visualize and understand patterns.

```{r}
spiders <-read.delim("S2_WolffData-Curated.txt",header = TRUE, stringsAsFactors = TRUE) #data importation
summary(spiders)
head(spiders)
```

The raw data set is composed of 473 observations of 24 variables.

## Data preprocessing

We don't want to analyze all species so we just keep the families. Also we keep relative variables instead of absolute ones in order to be able to compare spider families with different sizes. Data is not centered reduced yet.

Description of the quantitative variables :
  - rAME : relative Anterior Median Eyes
  - rALE : relative Anterior Lateral Eyes
  - rPME : relative Posterior Median Eyes
  - rPLE : relative Posterior Lateral Eyes

```{r}
spiders <- read.delim("S2_WolffData-Curated.txt", header = TRUE, stringsAsFactors = TRUE)
spiders <- spiders[c("Family","rAME","rALE","rPME","rPLE")] #data selection
spiders <- na.omit(spiders)
```

Let's center reduce.

```{r}
spiders$rALE <- (spiders$rALE-mean(spiders$rALE))/sd(spiders$rALE)
spiders$rAME <- (spiders$rAME-mean(spiders$rAME))/sd(spiders$rAME)
spiders$rPLE <- (spiders$rPLE-mean(spiders$rPLE))/sd(spiders$rPLE)
spiders$rPME <- (spiders$rPME-mean(spiders$rPME))/sd(spiders$rPME)
summary(spiders) #all means are equal to 0
```

## PCA Analysis

PCA Analysis

```{r}
library(FactoMineR)
<<<<<<< HEAD
library(factoextra)
=======
library("factoextra")
>>>>>>> dfbbab7 (ajout phrases sur dimensions et ajout de package factextra plus tot dans le code)
res.pca <- PCA(spiders,
               scale.unit = FALSE,    # already centered-reduced
               quali.sup = 1, # "Family" is qualitative and illustrative
               graph = FALSE)
```

Data visualisation

Graph of inertia :

```{r}
res.pca$eig
library(factoextra)
library(ggplot2)
fviz_eig(res.pca, ylim = c(0, 100), geom='bar', main='Percentage of inertia explained by axes')

```

Dimensions 1 explains 61.9% of total variance and dimensions 2 explains 24.2%, all together the two first dimensions explain 86.1% of the total inertia.

Graph of variables :

```{r}
fviz_pca_var(res.pca,
             col.var = "contrib",
             gradient.cols = c("blue", "yellow", "red"),
             repel = TRUE) #show how relative eyed sizes are correlated
```

Graph of individuals :

```{r}
fviz_pca_ind(res.pca, geom.ind = "point", # points uniquement 
             label = "none", # pas de texte sur les points 
             habillage = spiders$Family, # couleurs par famille (optionnel)
             legend = "none", # supprime complètement la légende 
             repel = TRUE)
```

```{r}
top_families <- names(sort(table(spiders$Family), decreasing = TRUE))[1:8] #only frequent families to have a clearer graph
spiders_sub <- subset(spiders, Family %in% top_families)
res.pca2 <- PCA(spiders_sub, scale.unit = FALSE, quali.sup = 1, graph = FALSE)
fviz_pca_ind(res.pca2, habillage = spiders_sub$Family, 
             #palette = "Dark2", 
             #repel = TRUE,
             select.ind = list(name = NULL, cos2 = NULL, contrib = NULL),
             label="none"
             )
```

Graph of contributions by variables :

```{r}
res.pca$var$contrib
```

Graph of contributions by individuals :

```{r}
#res.pca$ind$contrib

#Axis 1

top10_ind <- order(res.pca$ind$contrib[,1], decreasing = TRUE)[1:10] #the 10 individuals contributing the most to the first axis

top10_ind_fam <- data.frame(
  Individual = rownames(spiders)[top10_ind],
  Family = spiders$Family[top10_ind],
  Contribution = res.pca$ind$contrib[top10_ind, 1]
) #families associated to the individuals contributing the most

top10_ind_fam

#Axis 2
top10_ind_2 <- order(res.pca$ind$contrib[,2], decreasing = TRUE)[1:10] #the 10 individuals contributing the most to the first axis

top10_ind_fam_2 <- data.frame(
  Individual = rownames(spiders)[top10_ind_2],
  Family = spiders$Family[top10_ind_2],
  Contribution = res.pca$ind$contrib[top10_ind_2, 2]
) #families associated to the individuals contributing the most

top10_ind_fam_2

# Axis 1
head(sort(res.pca$ind$contrib[,1], decreasing = TRUE), 10)

# Axis 2
head(sort(res.pca$ind$contrib[,2], decreasing = TRUE), 10)
```

## Interpretation of the analysis

The two first axis explain more than 86% of the total variance so only two main components are selected for the analysis. The first dimension explains 61.9% of the variance while the second dimension explains 24.2% of the variance.

All variables are contributing a lot to the first axis. However, rALE and rPLE explain the best the axis with contributions higher than 30%. rAME and rPME explain the second axis with contributions respectively equal to 54% and 44%.

The individuals are mainly distributed along the first axis which can be interpreted as a gradient of variation in eye size. Individuals are also concentrated around zero which means that a majority of spiders has similar eye proportions.

When we visualise families, it looks like individuals from the *Salticidae* are atypical individuals because they are far on the first axis. In fact, those individuals have the highest contributions.

::: {style="text-align:center"}
![Figure 1 — Family *Salticidae* contributing the most to explained variance.](Salticidae.jpg){width="50%"}
:::
=======
fviz_pca_ind(res.pca2, habillage = spiders_sub$Family, palette = "Dark2", repel = TRUE)
```
Global visualisation (individuals+quantitative variables):
>>>>>>> 28b2eec (Data visualisation : individuals+variables graphs)
